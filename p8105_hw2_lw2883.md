homework 2
================
Leighanne Wang
9/24/2020

``` r
library(tidyverse)
```

    ## -- Attaching packages --------------------------------------------------------------------------------------------------------- tidyverse 1.3.0 --

    ## v ggplot2 3.3.2     v purrr   0.3.4
    ## v tibble  3.0.3     v dplyr   1.0.2
    ## v tidyr   1.1.2     v stringr 1.4.0
    ## v readr   1.3.1     v forcats 0.5.0

    ## -- Conflicts ------------------------------------------------------------------------------------------------------------ tidyverse_conflicts() --
    ## x dplyr::filter() masks stats::filter()
    ## x dplyr::lag()    masks stats::lag()

``` r
library(readxl)
library(haven)
```

## Problem 1

Define path to dataset

``` r
path_to_data_trash = "./data/Trash-Wheel-Collection-Totals-8-6-19.xlsx"
```

Read the Mr. Trashwheel dataset

``` r
trashwheel_df =
  read_xlsx(
    path = path_to_data_trash,
    sheet = "Mr. Trash Wheel",
    range = cell_cols("A:N")) %>% 
    janitor::clean_names() %>% 
    drop_na(dumpster) %>% 
    mutate(
      sports_balls = round(sports_balls),
      sports_balls = as.integer(sports_balls)
  )
```

Read and clean up the 2018 and 2017 precipitation data

``` r
precip_2018 =
  read_excel(
    path = path_to_data_trash,
    sheet = "2018 Precipitation",
    skip = 1
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2018) %>% 
  relocate(year)

precip_2017 =
  read_excel(
    path = path_to_data_trash,
    sheet = "2017 Precipitation",
    skip = 1
  ) %>% 
  janitor::clean_names() %>% 
  drop_na(month) %>% 
  mutate(year = 2017) %>% 
  relocate(year)
```

Combine annual precipitation

``` r
#create a df to change the name of the months in the combined annual precipitation dataset
month_df =
  tibble(
    month = 1:12,
    month_name = month.name
  )

#bind the two precipitation data from 2018 and 2017 to combine datasets
precip_df =
  bind_rows(precip_2018, precip_2017)

precip_df =
  left_join(precip_df, month_df, by = "month")
```

#### Trashwheel Dataset

The ‘trashwheel\_df’ dataset contains information from the
Mr. Trashwheel trash collector in Baltimore, Maryland from 2014 to
2019. It contains information on the number of dumpsters, date the trash
was collected (including month and year), the amount of trash collected
in weight (tons) and volume (cubic yards) and includes some types of
trash that was collected such as plastic bottles and grocery bags.
Finally, it also shows the number of homes powered from the collected
trash. There are a total of 344 observations in our final dataset.

  - The median number of sports balls found in the dumpster in 2017 was
    **8**.
  - The total weight of trash collected was **1122.45 tons** and
    **5347** cubic yards.
  - The total number of homes powered is **1.507583310^{4}**.

#### Precipitation Dataset

The ‘precip\_df’ dataset contains information for the annual
precipitation for the years 2017 and 2018 shown by month. There are a
total of 24 observations in our final dataset.

  - The total precipitation in 2018 was **70.33 inches**.
  - The total precipitation in 2017 was **32.93 inches**.

## Problem 2

Read the NYC Transit dataset

``` r
transit_df =
  read_csv("./data/NYC_Transit_Subway_Entrance_And_Exit_Data.csv") %>% 
    select(Line:Entry, Vending, ADA) %>% 
    janitor::clean_names() %>% 
    mutate(ada = as.logical(ada)  #mutate ada to logical
    )
```

    ## Parsed with column specification:
    ## cols(
    ##   .default = col_character(),
    ##   `Station Latitude` = col_double(),
    ##   `Station Longitude` = col_double(),
    ##   Route8 = col_double(),
    ##   Route9 = col_double(),
    ##   Route10 = col_double(),
    ##   Route11 = col_double(),
    ##   ADA = col_logical(),
    ##   `Free Crossover` = col_logical(),
    ##   `Entrance Latitude` = col_double(),
    ##   `Entrance Longitude` = col_double()
    ## )

    ## See spec(...) for full column specifications.

This dataset contains information about the NYC Subway Entrances and
Exits. It has information on the routes, subway lines and names, their
location (by latitude and longitude), if there is entry in that
location, the type of entrance it has, if there are vending machines,
and if it is ADA compliant. There are **1868 rows** and **19 columns**
in this dataset. The names have been cleaned in this dataset and the
‘ada’ variable has been changed from a character to a logical
variable. This data is not considered tidy because the subway route
numbers are individual columns, in order for this to be tidy, the subway
route number should be a variable and not individual column values.

  - There are **465** distinct stations.
  - There are **84** ADA compliant stations.
  - The proportion of station entrances/exits without vending that
    allows entrance is **0.0369379**.

**need to pivot\_longer to finish problem**

## Problem 3

Read the FiveThirtyEight data.

Read and clean pols-month.csv Separate ‘mon’ variable into ‘year’,
‘month’, ‘day’; create a ‘president’ variable from the ‘prez\_dem’ and
‘prez-gop’ variables and remove ‘day’ variable. **need to change month
number to month name**

``` r
pols_df =
  read_csv("./data/fivethirtyeight_datasets/pols-month.csv") %>% 
    janitor::clean_names() %>% 
    separate(mon, sep = "-", into = c("year", "month", "day")) %>% 
    pivot_longer(
      c(prez_gop, prez_dem),
      names_to = "president",
      names_prefix = "prez_"
    ) %>% 
    select(-day)
```

    ## Parsed with column specification:
    ## cols(
    ##   mon = col_date(format = ""),
    ##   prez_gop = col_double(),
    ##   gov_gop = col_double(),
    ##   sen_gop = col_double(),
    ##   rep_gop = col_double(),
    ##   prez_dem = col_double(),
    ##   gov_dem = col_double(),
    ##   sen_dem = col_double(),
    ##   rep_dem = col_double()
    ## )

Read and clean snp.csv Separate date into year, month, date; arrange by
year and month; relocate year and month to the front of dataset. **need
to change month number to month name**

``` r
snp_df =
  read_csv("./data/fivethirtyeight_datasets/snp.csv") %>% 
    janitor::clean_names() %>% 
    separate(date, sep = "/", into = c("month", "day", "year")) %>%
    arrange(year, month) %>% 
    relocate(year, month) %>% 
    select(-day)
```

    ## Parsed with column specification:
    ## cols(
    ##   date = col_character(),
    ##   close = col_double()
    ## )

Read, clean, and tidy unemployment.csv **need to change month to full
month name**

``` r
unemployment_df =
  read_csv("./data/fivethirtyeight_datasets/unemployment.csv") %>% 
    janitor::clean_names() %>% 
    pivot_longer(
      jan:dec,
      names_to = "month",
      values_to = "percentage"
    )
```

    ## Parsed with column specification:
    ## cols(
    ##   Year = col_double(),
    ##   Jan = col_double(),
    ##   Feb = col_double(),
    ##   Mar = col_double(),
    ##   Apr = col_double(),
    ##   May = col_double(),
    ##   Jun = col_double(),
    ##   Jul = col_double(),
    ##   Aug = col_double(),
    ##   Sep = col_double(),
    ##   Oct = col_double(),
    ##   Nov = col_double(),
    ##   Dec = col_double()
    ## )

We started off with 3 datasets: pols-month.csv, snp.csv, and
unemployment.csv.

  - “pols-month” contained the date (year and month), the number of
    republican and democatic senators, representatives, and presidents.
    It contained 1644 rows and 10 columns.
  - “snp” contained the date of observation (year and month) and the
    closing value of S\&P stock index on that date. It contained 787
    rows and 3 columns.  
  - “unemployment” contained the date (year and month) and the
    percentage of unemployment at that date. It contained 816 rows and 3
    columns.

The new dataset…
